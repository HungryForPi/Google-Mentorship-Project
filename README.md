# Google-Mentorship-Project (2023)

*Authors: Aditya Pahuja, Amber Shen, Calvin Zhang, Sophia Dasser*

This project is a website that translates the American Sign Language alphabet into English. The user signs the letters into a live streaming video on the website, and the corresponding English letters are shown in real-time. (also maybe add in a section like the abstract we had explaining it)

## ASL GESTURE RECOGNITION
### 1. **Tensorflow**
**What it is**: *summary of what tensorflow is*

**Usage**: We trained a Tensorflow Keras model with the ___ dataset. By feeding images from the dataset to train the model, we ended up with a ___ accuracy rate for images. To implement this in video, we used **Python OpenCV** to cut each video into frames, which the model would then recognize the video. 

### 2. **MediaPipe**
**What it is**: *summary of mediapipe*

**Usage**: We added the ASL alphabet as new gestures for MediaPipe to recognize. Then with **Python OpenCV**, we applied it to recognize the gestures in livestream data. 

## Website
We programmed the website using **HTML** and **CSS**. We implemented livestream video using ___. 

## Problems encountered 
- yes
- yes
- yes

**TODO â€” insert project description here at some point**

*Note.* When committing, it would be helpful if you prefaced the commit message
with (tensorflow) or (mediapipe), I think.
